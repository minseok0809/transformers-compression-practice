{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMnkeLbnhAip"
      },
      "source": [
        "# Model Size & Report Graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os \n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_path = './output/' + 'sst-2/' + 'quant/' + 'pytorch_model.bin' \n",
        "model = torch.load(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_path = './output/' + 'sst-2/' + 'quant/' + 'pytorch_model.bin' \n",
        "model = torch.load(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "key: bert.embeddings.word_embeddings.weight\n",
            "val.nelement: 23440896\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.embeddings.word_embeddings.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.embeddings.position_embeddings.weight\n",
            "val.nelement: 393216\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.embeddings.token_type_embeddings.weight\n",
            "val.nelement: 1536\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.embeddings.LayerNorm.weight\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.embeddings.LayerNorm.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.0.attention.self.clip_query\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.0.attention.self.clip_key\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.0.attention.self.clip_value\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.0.attention.self.clip_attn\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.0.attention.self.query.weight\n",
            "val.nelement: 589824\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.0.attention.self.query.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.0.attention.self.query.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.0.attention.self.query.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.0.attention.self.query.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.0.attention.self.key.weight\n",
            "val.nelement: 589824\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.0.attention.self.key.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.0.attention.self.key.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.0.attention.self.key.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.0.attention.self.key.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.0.attention.self.value.weight\n",
            "val.nelement: 589824\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.0.attention.self.value.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.0.attention.self.value.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.0.attention.self.value.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.0.attention.self.value.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.0.attention.output.gate\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.0.attention.output.dense.weight\n",
            "val.nelement: 589824\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.0.attention.output.dense.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.0.attention.output.dense.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.0.attention.output.dense.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.0.attention.output.dense.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.0.intermediate.dense.weight\n",
            "val.nelement: 2359296\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.0.intermediate.dense.bias\n",
            "val.nelement: 3072\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.0.intermediate.dense.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.0.intermediate.dense.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.0.intermediate.dense.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.0.output.gate\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.0.output.dense.weight\n",
            "val.nelement: 2359296\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.0.output.dense.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.0.output.dense.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.0.output.dense.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.0.output.dense.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.0.output.LayerNorm.weight\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.0.output.LayerNorm.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.1.attention.self.clip_query\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.1.attention.self.clip_key\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.1.attention.self.clip_value\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.1.attention.self.clip_attn\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.1.attention.self.query.weight\n",
            "val.nelement: 589824\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.1.attention.self.query.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.1.attention.self.query.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.1.attention.self.query.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.1.attention.self.query.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.1.attention.self.key.weight\n",
            "val.nelement: 589824\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.1.attention.self.key.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.1.attention.self.key.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.1.attention.self.key.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.1.attention.self.key.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.1.attention.self.value.weight\n",
            "val.nelement: 589824\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.1.attention.self.value.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.1.attention.self.value.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.1.attention.self.value.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.1.attention.self.value.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.1.attention.output.gate\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.1.attention.output.dense.weight\n",
            "val.nelement: 589824\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.1.attention.output.dense.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.1.attention.output.dense.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.1.attention.output.dense.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.1.attention.output.dense.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.1.intermediate.dense.weight\n",
            "val.nelement: 2359296\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.1.intermediate.dense.bias\n",
            "val.nelement: 3072\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.1.intermediate.dense.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.1.intermediate.dense.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.1.intermediate.dense.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.1.output.gate\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.1.output.dense.weight\n",
            "val.nelement: 2359296\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.1.output.dense.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.1.output.dense.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.1.output.dense.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.1.output.dense.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.1.output.LayerNorm.weight\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.1.output.LayerNorm.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.2.attention.self.clip_query\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.2.attention.self.clip_key\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.2.attention.self.clip_value\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.2.attention.self.clip_attn\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.2.attention.self.query.weight\n",
            "val.nelement: 589824\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.2.attention.self.query.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.2.attention.self.query.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.2.attention.self.query.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.2.attention.self.query.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.2.attention.self.key.weight\n",
            "val.nelement: 589824\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.2.attention.self.key.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.2.attention.self.key.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.2.attention.self.key.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.2.attention.self.key.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.2.attention.self.value.weight\n",
            "val.nelement: 589824\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.2.attention.self.value.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.2.attention.self.value.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.2.attention.self.value.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.2.attention.self.value.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.2.attention.output.gate\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.2.attention.output.dense.weight\n",
            "val.nelement: 589824\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.2.attention.output.dense.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.2.attention.output.dense.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.2.attention.output.dense.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.2.attention.output.dense.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.2.intermediate.dense.weight\n",
            "val.nelement: 2359296\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.2.intermediate.dense.bias\n",
            "val.nelement: 3072\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.2.intermediate.dense.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.2.intermediate.dense.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.2.intermediate.dense.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.2.output.gate\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.2.output.dense.weight\n",
            "val.nelement: 2359296\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.2.output.dense.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.2.output.dense.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.2.output.dense.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.2.output.dense.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.2.output.LayerNorm.weight\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.2.output.LayerNorm.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.3.attention.self.clip_query\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.3.attention.self.clip_key\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.3.attention.self.clip_value\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.3.attention.self.clip_attn\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.3.attention.self.query.weight\n",
            "val.nelement: 589824\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.3.attention.self.query.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.3.attention.self.query.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.3.attention.self.query.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.3.attention.self.query.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.3.attention.self.key.weight\n",
            "val.nelement: 589824\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.3.attention.self.key.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.3.attention.self.key.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.3.attention.self.key.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.3.attention.self.key.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.3.attention.self.value.weight\n",
            "val.nelement: 589824\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.3.attention.self.value.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.3.attention.self.value.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.3.attention.self.value.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.3.attention.self.value.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.3.attention.output.gate\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.3.attention.output.dense.weight\n",
            "val.nelement: 589824\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.3.attention.output.dense.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.3.attention.output.dense.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.3.attention.output.dense.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.3.attention.output.dense.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.3.intermediate.dense.weight\n",
            "val.nelement: 2359296\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.3.intermediate.dense.bias\n",
            "val.nelement: 3072\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.3.intermediate.dense.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.3.intermediate.dense.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.3.intermediate.dense.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.3.output.gate\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.3.output.dense.weight\n",
            "val.nelement: 2359296\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.3.output.dense.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.3.output.dense.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.3.output.dense.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.3.output.dense.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.3.output.LayerNorm.weight\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.3.output.LayerNorm.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.4.attention.self.clip_query\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.4.attention.self.clip_key\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.4.attention.self.clip_value\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.4.attention.self.clip_attn\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.4.attention.self.query.weight\n",
            "val.nelement: 589824\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.4.attention.self.query.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.4.attention.self.query.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.4.attention.self.query.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.4.attention.self.query.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.4.attention.self.key.weight\n",
            "val.nelement: 589824\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.4.attention.self.key.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.4.attention.self.key.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.4.attention.self.key.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.4.attention.self.key.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.4.attention.self.value.weight\n",
            "val.nelement: 589824\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.4.attention.self.value.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.4.attention.self.value.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.4.attention.self.value.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.4.attention.self.value.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.4.attention.output.gate\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.4.attention.output.dense.weight\n",
            "val.nelement: 589824\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.4.attention.output.dense.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.4.attention.output.dense.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.4.attention.output.dense.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.4.attention.output.dense.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.4.intermediate.dense.weight\n",
            "val.nelement: 2359296\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.4.intermediate.dense.bias\n",
            "val.nelement: 3072\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.4.intermediate.dense.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.4.intermediate.dense.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.4.intermediate.dense.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.4.output.gate\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.4.output.dense.weight\n",
            "val.nelement: 2359296\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.4.output.dense.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.4.output.dense.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.4.output.dense.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.4.output.dense.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.4.output.LayerNorm.weight\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.4.output.LayerNorm.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.5.attention.self.clip_query\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.5.attention.self.clip_key\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.5.attention.self.clip_value\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.5.attention.self.clip_attn\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.5.attention.self.query.weight\n",
            "val.nelement: 589824\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.5.attention.self.query.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.5.attention.self.query.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.5.attention.self.query.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.5.attention.self.query.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.5.attention.self.key.weight\n",
            "val.nelement: 589824\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.5.attention.self.key.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.5.attention.self.key.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.5.attention.self.key.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.5.attention.self.key.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.5.attention.self.value.weight\n",
            "val.nelement: 589824\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.5.attention.self.value.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.5.attention.self.value.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.5.attention.self.value.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.5.attention.self.value.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.5.attention.output.gate\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.5.attention.output.dense.weight\n",
            "val.nelement: 589824\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.5.attention.output.dense.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.5.attention.output.dense.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.5.attention.output.dense.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.5.attention.output.dense.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.5.intermediate.dense.weight\n",
            "val.nelement: 2359296\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.5.intermediate.dense.bias\n",
            "val.nelement: 3072\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.5.intermediate.dense.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.5.intermediate.dense.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.5.intermediate.dense.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.5.output.gate\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.5.output.dense.weight\n",
            "val.nelement: 2359296\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.5.output.dense.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.5.output.dense.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.5.output.dense.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.5.output.dense.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.5.output.LayerNorm.weight\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.5.output.LayerNorm.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.6.attention.self.clip_query\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.6.attention.self.clip_key\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.6.attention.self.clip_value\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.6.attention.self.clip_attn\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.6.attention.self.query.weight\n",
            "val.nelement: 589824\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.6.attention.self.query.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.6.attention.self.query.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.6.attention.self.query.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.6.attention.self.query.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.6.attention.self.key.weight\n",
            "val.nelement: 589824\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.6.attention.self.key.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.6.attention.self.key.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.6.attention.self.key.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.6.attention.self.key.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.6.attention.self.value.weight\n",
            "val.nelement: 589824\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.6.attention.self.value.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.6.attention.self.value.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.6.attention.self.value.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.6.attention.self.value.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.6.attention.output.gate\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.6.attention.output.dense.weight\n",
            "val.nelement: 589824\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.6.attention.output.dense.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.6.attention.output.dense.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.6.attention.output.dense.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.6.attention.output.dense.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.6.intermediate.dense.weight\n",
            "val.nelement: 2359296\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.6.intermediate.dense.bias\n",
            "val.nelement: 3072\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.6.intermediate.dense.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.6.intermediate.dense.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.6.intermediate.dense.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.6.output.gate\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.6.output.dense.weight\n",
            "val.nelement: 2359296\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.6.output.dense.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.6.output.dense.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.6.output.dense.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.6.output.dense.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.6.output.LayerNorm.weight\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.6.output.LayerNorm.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.7.attention.self.clip_query\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.7.attention.self.clip_key\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.7.attention.self.clip_value\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.7.attention.self.clip_attn\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.7.attention.self.query.weight\n",
            "val.nelement: 589824\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.7.attention.self.query.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.7.attention.self.query.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.7.attention.self.query.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.7.attention.self.query.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.7.attention.self.key.weight\n",
            "val.nelement: 589824\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.7.attention.self.key.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.7.attention.self.key.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.7.attention.self.key.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.7.attention.self.key.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.7.attention.self.value.weight\n",
            "val.nelement: 589824\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.7.attention.self.value.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.7.attention.self.value.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.7.attention.self.value.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.7.attention.self.value.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.7.attention.output.gate\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.7.attention.output.dense.weight\n",
            "val.nelement: 589824\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.7.attention.output.dense.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.7.attention.output.dense.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.7.attention.output.dense.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.7.attention.output.dense.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.7.intermediate.dense.weight\n",
            "val.nelement: 2359296\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.7.intermediate.dense.bias\n",
            "val.nelement: 3072\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.7.intermediate.dense.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.7.intermediate.dense.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.7.intermediate.dense.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.7.output.gate\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.7.output.dense.weight\n",
            "val.nelement: 2359296\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.7.output.dense.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.7.output.dense.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.7.output.dense.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.7.output.dense.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.7.output.LayerNorm.weight\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.7.output.LayerNorm.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.8.attention.self.clip_query\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.8.attention.self.clip_key\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.8.attention.self.clip_value\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.8.attention.self.clip_attn\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.8.attention.self.query.weight\n",
            "val.nelement: 589824\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.8.attention.self.query.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.8.attention.self.query.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.8.attention.self.query.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.8.attention.self.query.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.8.attention.self.key.weight\n",
            "val.nelement: 589824\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.8.attention.self.key.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.8.attention.self.key.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.8.attention.self.key.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.8.attention.self.key.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.8.attention.self.value.weight\n",
            "val.nelement: 589824\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.8.attention.self.value.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.8.attention.self.value.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.8.attention.self.value.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.8.attention.self.value.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.8.attention.output.gate\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.8.attention.output.dense.weight\n",
            "val.nelement: 589824\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.8.attention.output.dense.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.8.attention.output.dense.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.8.attention.output.dense.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.8.attention.output.dense.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.8.intermediate.dense.weight\n",
            "val.nelement: 2359296\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.8.intermediate.dense.bias\n",
            "val.nelement: 3072\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.8.intermediate.dense.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.8.intermediate.dense.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.8.intermediate.dense.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.8.output.gate\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.8.output.dense.weight\n",
            "val.nelement: 2359296\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.8.output.dense.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.8.output.dense.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.8.output.dense.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.8.output.dense.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.8.output.LayerNorm.weight\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.8.output.LayerNorm.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.9.attention.self.clip_query\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.9.attention.self.clip_key\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.9.attention.self.clip_value\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.9.attention.self.clip_attn\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.9.attention.self.query.weight\n",
            "val.nelement: 589824\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.9.attention.self.query.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.9.attention.self.query.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.9.attention.self.query.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.9.attention.self.query.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.9.attention.self.key.weight\n",
            "val.nelement: 589824\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.9.attention.self.key.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.9.attention.self.key.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.9.attention.self.key.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.9.attention.self.key.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.9.attention.self.value.weight\n",
            "val.nelement: 589824\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.9.attention.self.value.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.9.attention.self.value.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.9.attention.self.value.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.9.attention.self.value.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.9.attention.output.gate\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.9.attention.output.dense.weight\n",
            "val.nelement: 589824\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.9.attention.output.dense.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.9.attention.output.dense.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.9.attention.output.dense.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.9.attention.output.dense.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.9.intermediate.dense.weight\n",
            "val.nelement: 2359296\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.9.intermediate.dense.bias\n",
            "val.nelement: 3072\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.9.intermediate.dense.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.9.intermediate.dense.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.9.intermediate.dense.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.9.output.gate\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.9.output.dense.weight\n",
            "val.nelement: 2359296\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.9.output.dense.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.9.output.dense.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.9.output.dense.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.9.output.dense.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.9.output.LayerNorm.weight\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.9.output.LayerNorm.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.10.attention.self.clip_query\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.10.attention.self.clip_key\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.10.attention.self.clip_value\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.10.attention.self.clip_attn\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.10.attention.self.query.weight\n",
            "val.nelement: 589824\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.10.attention.self.query.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.10.attention.self.query.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.10.attention.self.query.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.10.attention.self.query.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.10.attention.self.key.weight\n",
            "val.nelement: 589824\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.10.attention.self.key.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.10.attention.self.key.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.10.attention.self.key.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.10.attention.self.key.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.10.attention.self.value.weight\n",
            "val.nelement: 589824\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.10.attention.self.value.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.10.attention.self.value.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.10.attention.self.value.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.10.attention.self.value.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.10.attention.output.gate\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.10.attention.output.dense.weight\n",
            "val.nelement: 589824\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.10.attention.output.dense.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.10.attention.output.dense.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.10.attention.output.dense.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.10.attention.output.dense.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.10.intermediate.dense.weight\n",
            "val.nelement: 2359296\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.10.intermediate.dense.bias\n",
            "val.nelement: 3072\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.10.intermediate.dense.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.10.intermediate.dense.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.10.intermediate.dense.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.10.output.gate\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.10.output.dense.weight\n",
            "val.nelement: 2359296\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.10.output.dense.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.10.output.dense.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.10.output.dense.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.10.output.dense.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.10.output.LayerNorm.weight\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.10.output.LayerNorm.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.11.attention.self.clip_query\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.11.attention.self.clip_key\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.11.attention.self.clip_value\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.11.attention.self.clip_attn\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.11.attention.self.query.weight\n",
            "val.nelement: 589824\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.11.attention.self.query.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.11.attention.self.query.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.11.attention.self.query.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.11.attention.self.query.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.11.attention.self.key.weight\n",
            "val.nelement: 589824\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.11.attention.self.key.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.11.attention.self.key.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.11.attention.self.key.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.11.attention.self.key.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.11.attention.self.value.weight\n",
            "val.nelement: 589824\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.11.attention.self.value.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.11.attention.self.value.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.11.attention.self.value.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.11.attention.self.value.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.11.attention.output.gate\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.11.attention.output.dense.weight\n",
            "val.nelement: 589824\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.11.attention.output.dense.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.11.attention.output.dense.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.11.attention.output.dense.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.11.attention.output.dense.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.11.intermediate.dense.weight\n",
            "val.nelement: 2359296\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.11.intermediate.dense.bias\n",
            "val.nelement: 3072\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.11.intermediate.dense.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.11.intermediate.dense.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.11.intermediate.dense.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.11.output.gate\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.11.output.dense.weight\n",
            "val.nelement: 2359296\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.11.output.dense.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.11.output.dense.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.11.output.dense.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.11.output.dense.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.11.output.LayerNorm.weight\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.encoder.layer.11.output.LayerNorm.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.pooler.dense.weight\n",
            "val.nelement: 589824\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.pooler.dense.bias\n",
            "val.nelement: 768\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.pooler.dense.scale\n",
            "val.nelement: 1\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.pooler.dense.weight_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: bert.pooler.dense.act_clip_val\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n",
            "key: classifier.weight\n",
            "val.nelement: 1536\n",
            "val.element_size: 4\n",
            "\n",
            "key: classifier.bias\n",
            "val.nelement: 2\n",
            "val.element_size: 4\n",
            "\n"
          ]
        }
      ],
      "source": [
        "num = 0 \n",
        "param_size = 0\n",
        "buffer_size = 0\n",
        "for key, val in model.items():\n",
        "    num += 1\n",
        "    print(\"key:\", key)\n",
        "    print(\"val.nelement:\", val.nelement())\n",
        "    print(\"val.element_size:\", val.element_size())\n",
        "    print()\n",
        "#  param_size += val.nelement() * val.element_size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size: 417.649 MB\n"
          ]
        }
      ],
      "source": [
        "param_size = 0\n",
        "buffer_size = 0\n",
        "for key, val in model.items():\n",
        "    param_size += val.nelement() * val.element_size()\n",
        "\n",
        "# for buffer in model.buffers():\n",
        "    # buffer_size += buffer.nelement() * buffer.element_size()\n",
        "\n",
        "size_all_mb = (param_size) / 1024**2\n",
        "print('Size: {:.3f} MB'.format(size_all_mb))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_bitwidth(tensor):\n",
        "    \"\"\"\n",
        "    A function to return the smallest bitwidth that can represent the tensor.\n",
        "    Specifically, if the max value of the tensor is smaller than 2 ** k - 1,\n",
        "    then the tensor can be packed with k bit.\n",
        "    \"\"\"\n",
        "    max_bitwidth = 8\n",
        "    max_value = tensor\n",
        "\n",
        "    for k in range(1, max_bitwidth + 1):\n",
        "        if max_value < 2 ** k - 1:\n",
        "            return k\n",
        "\n",
        "    print(\"Error: The tensor is more than 8-bit\")\n",
        "    return max_bitwidth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_bitwidth(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_optimal_compress_dim(tensor_shape, bit):\n",
        "    \"\"\"\n",
        "    A function to find the index of optimal dimemsion to apply packing on.\n",
        "\n",
        "    \"\"\"\n",
        "    index = 0\n",
        "    curr_index = 0\n",
        "    max_dim = 0\n",
        "    for dim in tensor_shape:\n",
        "        if (dim * bit) % 8 == 0:\n",
        "            index = curr_index\n",
        "            break\n",
        "        if dim > max_dim:\n",
        "            index = curr_index\n",
        "            max_dim = dim\n",
        "        curr_index += 1\n",
        "    return index\n",
        "\n",
        "def pack_tensor(tensor, bit):\n",
        "    \"\"\"\n",
        "    A function to pack tensor into a packed_tensor with only one dimension difference.\n",
        "    binary_tensor is the binary counterpart of tensor.\n",
        "    padded_binary_tensor is a vector reshaped from binary_tensor, it is padded with zeros\n",
        "    to have the same amount of values as the target binary_packed_tensor.\n",
        "    binary_packed_tensor is the binary counterpart of packed_tensor.\n",
        "    packed_tensor is the output tensor.\n",
        "\n",
        "    \"\"\"\n",
        "    tmp_tensor_shape = list(tensor.shape)\n",
        "    tmp_tensor_shape.append(1)\n",
        "    tmp_tensor = np.zeros(tuple(tmp_tensor_shape), dtype=np.uint8)\n",
        "    tmp_tensor[..., 0] = tensor\n",
        "\n",
        "    binary_tensor = np.unpackbits(tmp_tensor, axis=-1)\n",
        "    binary_tensor_shape = tmp_tensor_shape\n",
        "    binary_tensor_shape[-1] = 8\n",
        "\n",
        "    i = find_optimal_compress_dim(tensor.shape, bit)\n",
        "    packed_dim = int(tensor.shape[i] * bit / 8) + (tensor.shape[i] * bit % 8 > 0)\n",
        "    binary_packed_tensor_shape = binary_tensor_shape\n",
        "    binary_packed_tensor_shape[i] = packed_dim\n",
        "\n",
        "    packed_tensor = np.zeros(tuple(binary_packed_tensor_shape), dtype=np.uint8)\n",
        "    padded_binary_tensor = binary_tensor[..., 8-bit: 8].reshape((-1))\n",
        "    padding_width = packed_tensor.size - padded_binary_tensor.size\n",
        "    padded_binary_tensor = np.pad(padded_binary_tensor, (0, padding_width), 'constant', constant_values=(0, 0))\n",
        "    binary_packed_tensor = padded_binary_tensor.reshape(tuple(binary_packed_tensor_shape))\n",
        "\n",
        "    packed_tensor = np.packbits(binary_packed_tensor, axis=-1)\n",
        "    packed_tensor = packed_tensor[..., 0]\n",
        "\n",
        "    return packed_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[-1 -1 -1  1 -1 -1  1]\n",
            " [-1  1 -1  1  1 -1 -1]\n",
            " [-1 -1 -1 -1  1 -1 -1]\n",
            " [ 1 -1 -1 -1  1 -1  1]\n",
            " [ 1 -1  1  1  1  1  1]\n",
            " [ 1  1  1 -1 -1  1 -1]\n",
            " [-1 -1 -1 -1  1 -1  1]]\n",
            "(68, 68, 7, 7)\n",
            "[[255 255 255 255 255 255 255]\n",
            " [255 255 255 255 255 255 255]\n",
            " [255 255 255 255 255 255 255]\n",
            " [255 255 255 255 255 255 255]\n",
            " [255 255 255 255 255 255 255]\n",
            " [255 255 255 255 255 255 255]\n",
            " [255 255 255 255 255 255 255]]\n",
            "(9, 68, 7, 7)\n",
            "[[1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1 1]]\n",
            "(68, 68, 7, 7)\n"
          ]
        }
      ],
      "source": [
        "max_value = 2\n",
        "bit = 1\n",
        "assert max_value <= 2 ** bit\n",
        "\n",
        "tensor = (2*np.random.randint(max_value, size=(68, 68, 7, 7), dtype=np.int8)-1)\n",
        "print(tensor[0, 0, :, :])\n",
        "print(tensor.shape)\n",
        "\n",
        "packed_tensor = pack_tensor(tensor, bit)\n",
        "print(packed_tensor[0, 0, :, :])\n",
        "print(packed_tensor.shape)\n",
        "\n",
        "unpacked_tensor = unpack_tensor(packed_tensor, bit, (68, 68, 7, 7))\n",
        "print(unpacked_tensor[0, 0, :, :])\n",
        "print(unpacked_tensor.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "def unpack_tensor(packed_tensor, bit, target_shape):\n",
        "    \"\"\"\n",
        "    A function to unpack a packed_tensor into the original tensor with target_shape.\n",
        "    binary_packed_tensor is the binary counterpart of packed_tensor.\n",
        "    unpadded_binary_packed_tensor is a vector reshaped from binary_packed_tensor,\n",
        "    it is unpadded to have the same amount of values as binary_unpacked_tensor.\n",
        "    binary_unpacked_tensor is the binary counterpart of unpacked_tensor.\n",
        "    unpacked_tensor is the output tensor, it should be the same as the original tensor.\n",
        "\n",
        "    \"\"\"\n",
        "    tmp_packed_tensor_shape = list(packed_tensor.shape)\n",
        "    tmp_packed_tensor_shape.append(1)\n",
        "    tmp_packed_tensor = np.zeros(tuple(tmp_packed_tensor_shape), dtype=np.uint8)\n",
        "    tmp_packed_tensor[..., 0] = packed_tensor\n",
        "\n",
        "    binary_packed_tensor = np.unpackbits(tmp_packed_tensor, axis=-1).reshape((-1))\n",
        "    unpading_mask = np.arange(binary_packed_tensor.size - np.prod(target_shape) * bit) + np.prod(target_shape) * bit\n",
        "    unpadded_binary_packed_tensor = np.delete(binary_packed_tensor, unpading_mask)\n",
        "\n",
        "    binary_unpacked_tensor_shape = list(target_shape)\n",
        "    binary_unpacked_tensor_shape.append(bit)\n",
        "    unpadded_binary_packed_tensor = unpadded_binary_packed_tensor.reshape(tuple(binary_unpacked_tensor_shape))\n",
        "    binary_unpacked_tensor_shape[-1] = 8\n",
        "\n",
        "    binary_unpacked_tensor = np.zeros(tuple(binary_unpacked_tensor_shape), dtype=np.uint8)\n",
        "    binary_unpacked_tensor[..., 8-bit: 8] = unpadded_binary_packed_tensor[..., :]\n",
        "\n",
        "    unpacked_tensor = np.packbits(binary_unpacked_tensor, axis=-1).reshape(target_shape)\n",
        "\n",
        "    return unpacked_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "num = 0\n",
        "param_size = 0\n",
        "buffer_size = 0\n",
        "for key, val in model.items():\n",
        "    num += 1\n",
        "    param_size += val.nelement() * val.element_size()\n",
        "    if num == 1:\n",
        "        tensor = val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "unpacked_tensor = unpack_tensor(packed_tensor, 1)\n",
        "unpacked_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "param_size = 0\n",
        "buffer_size = 0\n",
        "for key, val in model['model_state_dict'].items():\n",
        "    param_size += val.nelement() * val.element_size()\n",
        "\n",
        "# for buffer in model.buffers():\n",
        "    # buffer_size += buffer.nelement() * buffer.element_size()\n",
        "\n",
        "size_all_mb = (param_size) / 1024**2\n",
        "print('Size: {:.3f} MB'.format(size_all_mb))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for key, val in model['model_state_dict'].items():\n",
        "    print(val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"parameters.txt\", \"a\") as f:\n",
        "    for key, val in model['model_state_dict'].items():\n",
        "        f.write(key + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"model.txt\", \"a\") as f:\n",
        "    for k, v in model.items():\n",
        "        source_list = []\n",
        "        source_list.append(\"\\n\")\n",
        "        source_list.append(str(k))\n",
        "        source_list.append(str(v))\n",
        "        f.write(\"\\n\".join(source_list))    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"model2.txt\", \"a\") as f:\n",
        "    for k, v in model.items():\n",
        "        source_list = []\n",
        "        source_list.append(\"\\n\")\n",
        "        source_list.append(str(k))\n",
        "        source_list.append(str(v))\n",
        "        f.write(\"\\n\".join(source_list))      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File Path: vit_cifar10/log_checkpoint/vit_cifar10/saves/vit_cifar10.49.pth.tar\n",
            "File Size: 41.12 MB\n",
            "File Path: vit_cifar10/log_checkpoint/quant2_vit_cifar10/saves/quant2_vit_cifar10.49.pth.tar\n",
            "File Size: 41.12 MB\n",
            "File Path: vit_cifar10/log_checkpoint/quant4_vit_cifar10/saves/quant4_vit_cifar10.49.pth.tar\n",
            "File Size: 41.12 MB\n",
            "File Path: vit_cifar10/log_checkpoint/quant8_vit_cifar10/saves/quant8_vit_cifar10.49.pth.tar\n",
            "File Size: 41.12 MB\n"
          ]
        }
      ],
      "source": [
        "def get_size(path):\n",
        "        size = os.path.getsize(path)\n",
        "        if size < 1024:\n",
        "            return f\"{size} bytes\"\n",
        "        elif size < pow(1024,2):\n",
        "            return f\"{round(size/1024, 2)} KB\"\n",
        "        elif size < pow(1024,3):\n",
        "            return f\"{round(size/(pow(1024,2)), 2)} MB\"\n",
        "        elif size < pow(1024,4):\n",
        "            return f\"{round(size/(pow(1024,3)), 2)} GB\"\n",
        "\n",
        "model_name = [model.replace('_log_df', '') for model in model_name]\n",
        "for model in model_name:\n",
        "    model_path = 'vit_cifar10/log_checkpoint/' + model + '/saves/' + model + '.49.pth.tar'\n",
        "    model_size = get_size(model_path) \n",
        "    print('File Path:', model_path)\n",
        "    print('File Size:', model_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File Path: vit_cifar10/log_checkpoint/all_quant8_vit_cifar10/saves/all_quant8_vit_cifar10.0.pth.tar\n",
            "File Size: 41.12 MB\n"
          ]
        }
      ],
      "source": [
        "model = \"all_quant8_vit_cifar10\"\n",
        "model_path = 'vit_cifar10/log_checkpoint/' + model + '/saves/' + model + '.0.pth.tar'\n",
        "model_size = get_size(model_path) \n",
        "print('File Path:', model_path)\n",
        "print('File Size:', model_size)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
